{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52e73d8d",
   "metadata": {},
   "source": [
    "# Predicting movie sales from Metacritic data\n",
    "\n",
    "## 0. Business problem\n",
    "\n",
    "The movie industry faces high financial-performance risks because of increasingly high movie-making and marketing costs and a high degree of uncertainty about audience reactions (Escoffier et al., 2015).\n",
    "While Metacritic provides rich information about movies such as the critic scores, user scores, review texts and metadata, it is \n",
    "unclear how well these features can predict the monetary success. That's why this project uses historical data from Metacritic and movie sales information information to build several machine learning models that forecast whether a movie will result in low, medium or high sales. Furthermore, this project will focus on explaining which features drive these predictions. \n",
    "The final goal is to provide the movie publisher with valuable information on where to spend the marketing budget.\n",
    "\n",
    "**Business Question** How can we predict box office perfomance of the movie to accuratly allocate marketing budget?\n",
    "\n",
    "**Source**:\n",
    "Escoffier, N., & McKelvey, B. (2015). The Wisdom of Crowds in the Movie Industry: Towards New Solutions to Reduce Uncertainties. International Journal of Arts Management, 17(2), 52–63. http://www.jstor.org/stable/24587073\n",
    "\n",
    "### 0.1 Main research question & subquestions\n",
    "**Main research question**:\n",
    "How accurately can we predict a movie's box-office sales using Metacritic ratings, metadata, review texts, with particular focus on identifying the most influential predictive factors?\n",
    "\n",
    "**Subquestions**\n",
    "1. How are critic scores, user scores, genres, platforms, and release years related to the sales tiers of movies?\n",
    "2. How well can different machine learning models predict the sales tier of a movie, based on structured features?\n",
    "3. To what extent does adding transformers of review titles and/or movie summaries improve prediction performance compared to models using only structured features? \n",
    "4. Which features are most influential in predicting high versus low sales according to SHAP?\n",
    "5. Can we identify review topics and/or movie clusters (e.g., using BERTopic and clustering methods) that are particularly associated with high or low sales tiers, and do these insights reveal distinct market segments?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cf3699",
   "metadata": {},
   "source": [
    "The movie sales prediction dataset is contained in the dataset folder in the repository. We will read the data and clean it to make it ready for analysis.\n",
    "\n",
    "The following information is provided on the dataset variables selected to address the research questions:\n",
    "\n",
    "This research employs a continuous numerical variable, **Worldwide Box Office**, as the response variable. This represents the total revenue generated globally (in USD).\n",
    "\n",
    "This study reviewed the literature and used the following 10 variables as explanatory variables:\n",
    "\n",
    "- **X1**: Metascore\n",
    "  - A weighted average of critic reviews (Scale: 0 - 100).\n",
    "- **X2**: Userscore\n",
    "  - Average score provided by general users (Scale: 0 - 10).\n",
    "- **X3**: Production Budget\n",
    "  - The estimated financial cost to produce the film (USD).\n",
    "- **X4**: Genre\n",
    "  - Categorical variable indicating the primary classification of the movie (e.g., Action, Comedy, Drama).\n",
    "  - Movies with multiple genres are processed using One-Hot Encoding.\n",
    "- **X5**: Release Date\n",
    "  - Used to extract the specific month and year of release to account for seasonal market trends and inflation adjustments.\n",
    "- **X6**: Runtime\n",
    "  - The duration of the movie in minutes.\n",
    "- **X7**: Theatre Count\n",
    "  - The number of theatres showing the movie during its opening weekend, serving as a proxy for distribution width.\n",
    "- **X8**: MPAA Rating\n",
    "  - Categorical certification defining the target audience scope:\n",
    "    - G = General Audiences\n",
    "    - PG = Parental Guidance Suggested\n",
    "    - PG-13 = Parents Strongly Cautioned\n",
    "    - R = Restricted\n",
    "    - NC-17 = Adults Only\n",
    "- **X9**: Movie Summary\n",
    "  - The textual plot summary of the film.\n",
    "  - Used to generate semantic embeddings via Transformers to capture narrative elements.\n",
    "- **X10**: Review Text\n",
    "  - The raw text body of expert and user reviews.\n",
    "  - Used for BERTopic modeling to identify dominant discourse topics associated with sales performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d886156",
   "metadata": {},
   "source": [
    "## 1. EDA (exploratory data analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daaa639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" # to make jupyter print all outputs, not just the last one\n",
    "from IPython.core.display import HTML # to pretty print pandas df and be able to copy them over (e.g. to ppt slides)\n",
    "import re, hashlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e714037e",
   "metadata": {},
   "source": [
    "### 1.1 Basic structure of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bff2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define file paths relative to the notebook\n",
    "data_folder = \"datasets\"\n",
    "\n",
    "sales_path = os.path.join(data_folder, \"sales.xlsx\")\n",
    "userreviews_path = os.path.join(data_folder, \"UserReviews.xlsx\")\n",
    "expertreviews_path = os.path.join(data_folder, \"ExpertReviews.xlsx\")\n",
    "meta_path = os.path.join(data_folder, \"metaClean43Brightspace.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059eae5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the four Excel files\n",
    "UserReviews_raw = pd.read_excel(userreviews_path)\n",
    "ExpertReviews_raw = pd.read_excel(expertreviews_path)\n",
    "sales_raw       = pd.read_excel(sales_path)\n",
    "meta_raw        = pd.read_excel(meta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a5e1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {} rows and {} columns in the user review dataset\".format(UserReviews_raw.shape[0], UserReviews_raw.shape[1]))\n",
    "print(\"There are {} rows and {} columns in the expert review dataset\".format(ExpertReviews_raw.shape[0], ExpertReviews_raw.shape[1]))\n",
    "print(\"There are {} rows and {} columns in the sales dataset\".format(sales_raw.shape[0], sales_raw.shape[1]))\n",
    "print(\"There are {} rows and {} columns in the meta dataset\".format(meta_raw.shape[0], meta_raw.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1bf9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"UserReview \")\n",
    "UserReviews_raw.head()\n",
    "UserReviews_raw.tail()\n",
    "print(\"Expert Review\")\n",
    "ExpertReviews_raw.head()\n",
    "ExpertReviews_raw.tail()\n",
    "print(\"Meta data\")\n",
    "meta_raw.head()\n",
    "meta_raw.tail()\n",
    "print(\"Sales data\")\n",
    "sales_raw.head()\n",
    "sales_raw.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af79ca5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_basic_info(name, df):\n",
    "    print(f\"\\n{name}\")\n",
    "    print(\"-\" * len(name))\n",
    "    print(\"Columns:\", list(df.columns))\n",
    "    print(\"\\nMissing values per column:\")\n",
    "    display(df.isna().sum().sort_values(ascending=False).head(15))\n",
    "\n",
    "print_basic_info(\"UserReviews_raw\", UserReviews_raw)\n",
    "print_basic_info(\"ExpertReviews_raw\", ExpertReviews_raw)\n",
    "print_basic_info(\"sales_raw\", sales_raw)\n",
    "print_basic_info(\"meta_raw\", meta_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3354804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_mixed_types(df, df_name):\n",
    "    \"\"\"\n",
    "    Check for columns with mixed data types in a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: The DataFrame to check\n",
    "        df_name: Name of the DataFrame (for display purposes)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Checking: {df_name}\")\n",
    "    print('='*50)\n",
    "    \n",
    "    found_mixed = False\n",
    "    \n",
    "    for col in df.columns:\n",
    "        \n",
    "        # Map type names to more readable versions\n",
    "        # NaN values show as 'float', so we relabel them as 'NaN/missing'\n",
    "        def get_type(x):\n",
    "            if pd.isna(x):\n",
    "                return 'NaN/missing'\n",
    "            return type(x).__name__\n",
    "        \n",
    "        types = df[col].apply(get_type).value_counts().to_dict()\n",
    "        \n",
    "        # Only flag columns with actual mixed types (ignoring NaN)\n",
    "        non_nan_types = {k: v for k, v in types.items() if k != 'NaN/missing'}\n",
    "        \n",
    "        if len(non_nan_types) > 1:\n",
    "            found_mixed = True\n",
    "            type_parts = [f\"{count} {dtype}\" for dtype, count in types.items()]\n",
    "            print(f\"  '{col}' has mixed types: {', '.join(type_parts)}\")\n",
    "    \n",
    "    if not found_mixed:\n",
    "        print(\"  No mixed types found - all columns are clean!\")\n",
    "\n",
    "check_mixed_types(UserReviews_raw, \"UserReviews_raw\")\n",
    "check_mixed_types(ExpertReviews_raw, \"ExpertReviews_raw\")\n",
    "check_mixed_types(sales_raw, \"sales_raw\")\n",
    "check_mixed_types(meta_raw, \"meta_raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c719ef38",
   "metadata": {},
   "source": [
    "### Reflection on the raw datasets\n",
    "\n",
    "Based on the first EDA and the 10 variables (X1-X10) identified for this research, we can see which problems exist in each table and what cleaning steps are needed.\n",
    "\n",
    "#### Required Variables Overview\n",
    "\n",
    "Our research focuses on 10 specific variables:\n",
    "- **X1**: Metascore (from `meta_raw`)  \n",
    "- **X2**: Userscore (from `meta_raw`)  \n",
    "- **X3**: Production Budget (from `sales_raw`)  \n",
    "- **X4**: Genre (from `meta_raw`)  \n",
    "- **X5**: Release Date (from `meta_raw` or `sales_raw`)  \n",
    "- **X6**: Runtime (from `meta_raw`)  \n",
    "- **X7**: Theatre Count (from `sales_raw`)  \n",
    "- **X8**: MPAA Rating (from `meta_raw`)  \n",
    "- **X9**: Movie Summary (from `meta_raw`)  \n",
    "- **X10**: Review Text (from `UserReviews_raw` and `ExpertReviews_raw`)  \n",
    "- **Target**: Worldwide Box Office (from `sales_raw`)\n",
    "\n",
    "#### meta_raw\n",
    "\n",
    "- Shape: 11,364 rows and 13 columns. This is our primary movie-level metadata table.  \n",
    "- **Critical columns for X1-X2, X4, X5-X6, X8-X9**:  \n",
    "  - `metascore` (X1): No missing values, already numeric. This is our primary critic rating.  \n",
    "  - `userscore` (X2): Contains some missing values, needs numeric conversion.  \n",
    "  - `genre` (X4): Contains genre information, will need One-Hot Encoding for multiple genres.  \n",
    "  - `RelDate` (X5): Already in clean date format, no missing values. Will extract month and year.  \n",
    "  - `runtime` (X6): Contains movie duration in minutes, needs numeric cleaning.  \n",
    "  - `rating` (X8): MPAA rating (G, PG, PG-13, R, NC-17), categorical variable.  \n",
    "  - `summary` (X9): Plot summary text, many missing values but essential for semantic embeddings.  \n",
    "  - `url`: No missing values, serves as our primary linking key.\n",
    "\n",
    "Cleaning implications:\n",
    "- Standardise `RelDate` into a single `releasedate` column using `standardize_meta_dates`.  \n",
    "- Ensure `metascore`, `userscore` and `runtime` are properly numeric.  \n",
    "- Keep `summary` text for later transformer-based embeddings, noting missing values.  \n",
    "- Process `genre` for One-Hot Encoding.  \n",
    "- Validate `rating` contains only valid MPAA categories.\n",
    "\n",
    "#### sales_raw\n",
    "\n",
    "- Shape: 30,612 rows and 16 columns. This contains our target variable and X3, X5, X7.  \n",
    "- **Critical columns for X3, X5, X7 and target**:  \n",
    "  - `worldwide_box_office`: Our target variable, comes as text and needs numeric conversion.  \n",
    "  - `production_budget` (X3): Many missing values (~20,000), needs numeric conversion from text.  \n",
    "  - `theatre_count` (X7): Distribution width proxy, many missing values, needs numeric conversion.  \n",
    "  - `release_date` (X5): Free-form text date needs conversion to datetime for seasonal trends.  \n",
    "  - `url`: No missing values, used for linking to meta table.  \n",
    "- Column `Unnamed: 8` is completely empty and should be dropped.\n",
    "\n",
    "Cleaning implications:\n",
    "- Convert `worldwide_box_office`, `production_budget` and `theatre_count` to numeric using `get_numeric_value`.  \n",
    "- Convert `release_date` to datetime via `clean_sales_dates`, with fallback on `year` where needed.  \n",
    "- Handle missing values in budget and theatre count appropriately for modelling.  \n",
    "- Drop irrelevant columns like `Unnamed: 8`.\n",
    "\n",
    "#### UserReviews_raw & ExpertReviews_raw\n",
    "\n",
    "- Combined shapes: 319,662 user reviews (7 columns) + 238,973 expert reviews (5 columns).  \n",
    "- **Critical column for X10**:  \n",
    "  - `Rev`: Review text body, essential for BERTopic modeling and identifying discourse topics.  \n",
    "  - `url`: Links reviews to specific movies for aggregation.  \n",
    "  - `dateP`: Review dates, though not in our X1-X10, useful for temporal analysis.  \n",
    "- User reviews have ~3,400 missing values in reviewer and date fields.  \n",
    "- Expert reviews have only 2 missing values across key fields.\n",
    "\n",
    "Cleaning implications:\n",
    "- Primary focus: ensure `Rev` text is preserved and linked to correct movies via `url`.  \n",
    "- Convert dates to datetime format for consistency.  \n",
    "- Clean reviewer names (replace missing with \"Unknown Reviewer\").  \n",
    "- Create review IDs for tracking individual reviews.  \n",
    "- Plan to aggregate reviews at movie level or use for BERTopic analysis.\n",
    "\n",
    "#### Across all tables\n",
    "\n",
    "- All tables have a `url` column with no missing values, making it our natural linking key.  \n",
    "- However, URLs may have slight variations, so we need standardisation to create a consistent `movie_id`.  \n",
    "- After cleaning, we'll have a movie-level dataset with all X1-X10 variables aligned.\n",
    "\n",
    "Helper functions needed:\n",
    "- `clean_movie_name` and `extract_title_from_url` to standardise movie identifiers.  \n",
    "- `collect_all_movie_records`, `create_movie_dimension_table` and `add_movie_ids_to_datasets` to create a movie dimension table.  \n",
    "- Data type conversion helpers for numeric fields (budget, scores, runtime, theatre count).  \n",
    "- Date standardisation functions for release dates across tables.\n",
    "\n",
    "After these cleaning steps, we'll have:\n",
    "- A clean movie-level dataset with all 10 predictor variables (X1-X10) and the target (Worldwide Box Office).  \n",
    "- Consistent date formats for temporal feature extraction (month, year).  \n",
    "- Properly formatted numeric and categorical variables ready for One-Hot Encoding.  \n",
    "- Review texts prepared for transformer embeddings and topic modeling.  \n",
    "- All tables linkable via `movie_id` for seamless joins.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64de5d9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afsvoo3ifg",
   "metadata": {},
   "source": [
    "### 1.2 Data Cleaning Implementation\n",
    "\n",
    "Now we implement the cleaning steps outlined in the reflection above. We'll create reusable helper functions following best practices from the tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jnozyzv65p9",
   "metadata": {},
   "source": [
    "#### 1: Extracting numeric values from strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89177b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric conversion helper function\n",
    "def get_numeric_value(value):\n",
    "    # Handle missing values and text placeholders\n",
    "    if pd.isna(value) or (isinstance(value, str) and value.strip().lower() in ['', 'n/a', 'na', 'none', 'tbd']):\n",
    "        return np.nan\n",
    "    # Already numeric - just convert to float\n",
    "    if isinstance(value, (int, float)):\n",
    "        return float(value)\n",
    "    # Remove currency symbols, commas, spaces, percentages and convert\n",
    "    try:\n",
    "        return float(re.sub(r'[$,€£¥\\s%]', '', str(value).strip()))\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "# Apply to meta dataset (X1, X2, X6)\n",
    "meta_clean = meta_raw.copy()\n",
    "meta_clean[['metascore', 'userscore', 'runtime']] = meta_clean[['metascore', 'userscore', 'runtime']].applymap(get_numeric_value)\n",
    "\n",
    "# Apply to sales dataset (target, X3, X7)\n",
    "sales_clean = sales_raw.drop(columns=['Unnamed: 8'], errors='ignore')\n",
    "sales_clean[['worldwide_box_office', 'production_budget', 'theatre_count']] = sales_clean[['worldwide_box_office', 'production_budget', 'theatre_count']].applymap(get_numeric_value)\n",
    "\n",
    "# Apply to user reviews (not used in X1-X10 but cleaned for consistency)\n",
    "UserReviews_clean = UserReviews_raw.copy()\n",
    "UserReviews_clean['idvscore'] = UserReviews_clean['idvscore'].apply(get_numeric_value)\n",
    "\n",
    "# Apply to expert reviews (not used in X1-X10 but cleaned for consistency)\n",
    "ExpertReviews_clean = ExpertReviews_raw.copy()\n",
    "ExpertReviews_clean['idvscore'] = ExpertReviews_clean['idvscore'].apply(get_numeric_value)\n",
    "\n",
    "print(f\"✓ Numeric cleaning complete: meta({len(meta_clean)}), sales({len(sales_clean)}), user({len(UserReviews_clean)}), expert({len(ExpertReviews_clean)})\")\n",
    "\n",
    "# Verification\n",
    "print(f\"Types: metascore={meta_clean['metascore'].dtype}, worldwide_box_office={sales_clean['worldwide_box_office'].dtype}\")\n",
    "print(f\"Ranges: metascore[{meta_clean['metascore'].min():.0f}-{meta_clean['metascore'].max():.0f}], box_office[${sales_clean['worldwide_box_office'].min():,.0f}-${sales_clean['worldwide_box_office'].max():,.0f}]\")\n",
    "print(f\"Missing: metascore={meta_clean['metascore'].isna().sum()}, production_budget={sales_clean['production_budget'].isna().sum()}, theatre_count={sales_clean['theatre_count'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac979c83",
   "metadata": {},
   "source": [
    "#### 2: Date conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad112a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date conversion\n",
    "def parse_date_vectorized(date_series, fallback_year_series=None):\n",
    "    # Step 1: Remove ordinal suffixes from all dates at once (e.g., \"January 1st\" -> \"January 1\")\n",
    "    cleaned = date_series.astype(str).str.replace(r'(\\d+)(st|nd|rd|th)', r'\\1', regex=True)\n",
    "    \n",
    "    # Step 2: Add fallback year to dates without a 4-digit year\n",
    "    if fallback_year_series is not None:\n",
    "        mask = ~cleaned.str.contains(r'\\d{4}', na=False)  # Find dates without year\n",
    "        cleaned = cleaned.where(~mask, cleaned + ', ' + fallback_year_series.astype(str))\n",
    "    \n",
    "    # Step 3: Convert to datetime, invalid dates become NaT\n",
    "    return pd.to_datetime(cleaned, errors='coerce')\n",
    "\n",
    "# Apply to meta dataset (X5) - RelDate already in clean format, just convert\n",
    "meta_clean['releasedate'] = pd.to_datetime(meta_clean['RelDate'], errors='coerce')\n",
    "\n",
    "# Apply to sales dataset (X5) - use 'year' column as fallback for incomplete dates\n",
    "sales_clean['releasedate'] = parse_date_vectorized(sales_clean['release_date'], sales_clean['year'])\n",
    "\n",
    "# Apply to review datasets - already in standard format, direct conversion\n",
    "UserReviews_clean['dateP'] = pd.to_datetime(UserReviews_clean['dateP'], errors='coerce')\n",
    "ExpertReviews_clean['dateP'] = pd.to_datetime(ExpertReviews_clean['dateP'], errors='coerce')\n",
    "\n",
    "print(f\" Date cleaning complete: all datasets have standardized datetime columns\")\n",
    "\n",
    "# Verification: check date ranges and missing values\n",
    "print(f\" Date ranges: meta[{meta_clean['releasedate'].min()} to {meta_clean['releasedate'].max()}], sales[{sales_clean['releasedate'].min()} to {sales_clean['releasedate'].max()}]\")\n",
    "print(f\" Missing dates: meta={meta_clean['releasedate'].isna().sum()}, sales={sales_clean['releasedate'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6710b4d3",
   "metadata": {},
   "source": [
    "#### 3. Movie linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ce30d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_movie_name(text):\n",
    "    \"\"\"Make movie names consistent for matching across datasets.\"\"\"\n",
    "    if pd.isna(text) or text is None:\n",
    "        return \"\"\n",
    "    name = str(text).lower().strip().rstrip(\"/\")\n",
    "    name = name.rsplit(\"/\", 1)[-1]  # take last part after slash (for URLs)\n",
    "    name = name.replace(\"-\", \" \").replace(\"_\", \" \")\n",
    "    # remove brackets content\n",
    "    name = re.sub(r\"\\([^)]*\\)|\\[[^\\]]*\\]\", \"\", name)\n",
    "    # remove year at end\n",
    "    name = re.sub(r\"[\\(\\[]\\s*(?:19|20)\\d{2}[^)\\]]*\\s*[\\)\\]]\\s*$|(?:19|20)\\d{2}\\s*$\", \"\", name)\n",
    "    # keep only letters, digits, spaces\n",
    "    name = re.sub(r\"[^a-z0-9\\s\\-']\", \"\", name)\n",
    "    # collapse spaces\n",
    "    name = re.sub(r\"\\s+\", \" \", name).strip()\n",
    "    return name\n",
    "\n",
    "# Create cleaned names and movie_id for both datasets\n",
    "meta_clean['cleaned_name'] = meta_clean['url'].apply(clean_movie_name)\n",
    "meta_clean['movie_id'] = meta_clean['cleaned_name'].apply(lambda x: hashlib.md5(x.encode()).hexdigest()[:12] if x else None)\n",
    "\n",
    "sales_clean['cleaned_name'] = sales_clean['url'].apply(clean_movie_name)\n",
    "sales_clean['movie_id'] = sales_clean['cleaned_name'].apply(lambda x: hashlib.md5(x.encode()).hexdigest()[:12] if x else None)\n",
    "\n",
    "\n",
    "# Check overlap\n",
    "meta_ids = set(meta_clean['movie_id'].dropna())\n",
    "sales_ids = set(sales_clean['movie_id'].dropna())\n",
    "overlap = len(meta_ids.intersection(sales_ids))\n",
    "print(f\"✓ Overlapping movies: {overlap}\")\n",
    "print(f\"  Meta unique: {len(meta_ids)}, Sales unique: {len(sales_ids)}\")\n",
    "\n",
    "# Merge datasets - using sales as main source with LEFT JOIN\n",
    "# This keeps ALL movies from sales, even if they don't have metadata\n",
    "movie_data = sales_clean.merge(meta_clean, on='movie_id', how='left', suffixes=('_sales', '_meta'))\n",
    "print(f\"✓ Merged dataset: {len(movie_data)} movies (all from sales dataset)\")\n",
    "print(f\"  Movies with metadata: {movie_data['metascore'].notna().sum()}\")\n",
    "print(f\"  Movies without metadata: {movie_data['metascore'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed43fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since sales is now the base table, we need to handle column selection differently\n",
    "# Columns from sales (base) don't have suffixes\n",
    "# Columns from meta have _meta suffix\n",
    "# For overlapping columns (genre, runtime, releasedate), prefer meta version when available, otherwise use sales\n",
    "\n",
    "# First, let's create consolidated columns for the overlapping fields\n",
    "# For genre: prefer meta (more complete), fallback to sales\n",
    "movie_data['genre'] = movie_data['genre_meta'].fillna(movie_data['genre_sales'])\n",
    "\n",
    "# For releasedate: prefer sales (already cleaned), fallback to meta\n",
    "movie_data['releasedate'] = movie_data['releasedate_sales'].fillna(movie_data['releasedate_meta'])\n",
    "\n",
    "# For runtime: prefer meta (more accurate), fallback to sales\n",
    "movie_data['runtime'] = movie_data['runtime_meta'].fillna(movie_data['runtime_sales'])\n",
    "\n",
    "# Now select final columns\n",
    "movie_data = movie_data[[\n",
    "    'movie_id',\n",
    "    'metascore',              # X1 (from meta)\n",
    "    'userscore',              # X2 (from meta)\n",
    "    'production_budget',      # X3 (from sales, no suffix)\n",
    "    'genre',                  # X4 (consolidated)\n",
    "    'releasedate',            # X5 (consolidated)\n",
    "    'runtime',                # X6 (consolidated)\n",
    "    'theatre_count',          # X7 (from sales, no suffix)\n",
    "    'rating',                 # X8 (from meta)\n",
    "    'summary',                # X9 (from meta)\n",
    "    'worldwide_box_office'    # Target (from sales, no suffix)\n",
    "    # X10 (review text) will be aggregated separately\n",
    "]]\n",
    "\n",
    "# Remove movies with missing movie_id (mostly year-only titles that can't be matched)\n",
    "print(f\"Removing {movie_data['movie_id'].isna().sum()} movies with missing movie_id...\")\n",
    "movie_data = movie_data[movie_data['movie_id'].notna()].copy()\n",
    "\n",
    "print(f\"✓ Movie-level dataset: {len(movie_data)} movies with X1-X9 + target\")\n",
    "print(f\"\\n✓ Missing values:\")\n",
    "print(movie_data.isnull().sum())\n",
    "print(f\"\\n✓ Complete cases (no missing values): {movie_data.dropna().shape[0]} movies\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5f8c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only movies with box office data (target variable)\n",
    "movie_data_with_target = movie_data[movie_data['worldwide_box_office'].notna()].copy()\n",
    "print(f\"✓ Movies with box office data: {len(movie_data_with_target)}\")\n",
    "\n",
    "# Now add movie_id to review datasets\n",
    "UserReviews_clean['cleaned_name'] = UserReviews_clean['url'].apply(clean_movie_name)\n",
    "UserReviews_clean['movie_id'] = UserReviews_clean['cleaned_name'].apply(lambda x: hashlib.md5(x.encode()).hexdigest()[:12] if x else None)\n",
    "\n",
    "ExpertReviews_clean['cleaned_name'] = ExpertReviews_clean['url'].apply(clean_movie_name)\n",
    "ExpertReviews_clean['movie_id'] = ExpertReviews_clean['cleaned_name'].apply(lambda x: hashlib.md5(x.encode()).hexdigest()[:12] if x else None)\n",
    "\n",
    "print(f\"✓ Added movie_id to reviews\")\n",
    "\n",
    "# Aggregate review texts\n",
    "user_reviews_agg = UserReviews_clean.groupby('movie_id')['Rev'].apply(lambda x: ' '.join(x.dropna().astype(str))).reset_index()\n",
    "user_reviews_agg.columns = ['movie_id', 'user_review_text']\n",
    "\n",
    "expert_reviews_agg = ExpertReviews_clean.groupby('movie_id')['Rev'].apply(lambda x: ' '.join(x.dropna().astype(str))).reset_index()\n",
    "expert_reviews_agg.columns = ['movie_id', 'expert_review_text']\n",
    "\n",
    "# Merge everything\n",
    "movie_data_complete = movie_data_with_target.merge(user_reviews_agg, on='movie_id', how='left')\n",
    "movie_data_complete = movie_data_complete.merge(expert_reviews_agg, on='movie_id', how='left')\n",
    "\n",
    "movie_data_complete['review_text'] = (\n",
    "    movie_data_complete['user_review_text'].fillna('') + ' ' + \n",
    "    movie_data_complete['expert_review_text'].fillna('')\n",
    ").str.strip()\n",
    "\n",
    "movie_data_complete['release_month'] = movie_data_complete['releasedate'].dt.month\n",
    "movie_data_complete['release_year'] = movie_data_complete['releasedate'].dt.year\n",
    "\n",
    "print(f\"✓ Final dataset: {len(movie_data_complete)} movies with all X1-X10 variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4990258",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83635eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data_complete.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fbe007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# again show the information on the the movie_data_complete dataframe\n",
    "\n",
    "print(f\"✓ Movie-level dataset: {len(movie_data_complete)} movies with X1-X9 + target\")\n",
    "print(f\"\\n✓ Missing values:\")\n",
    "print(movie_data_complete.isnull().sum())\n",
    "print(f\"\\n✓ Complete cases (no missing values): {movie_data_complete.dropna().shape[0]} movies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9055c635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt #for creating more graphical illustrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f5d3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a fresh dataframe so the movie data is not fragmented and we dont get that warning\n",
    "movie_data_complete = movie_data_complete.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234d712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EDA_plots(df, column, target='worldwide_box_office'):\n",
    "    \"\"\"\n",
    "    EDA for a single numeric feature (because this is NOT a classifaction problem, but a (linear) regression problem).\n",
    "\n",
    "    Plots:\n",
    "    1. Histogram of the feature\n",
    "    2. Boxplot of the feature\n",
    "    3. KDE distribution of the feature\n",
    "    4. Scatterplot: feature vs target (e.g. worldwide_box_office)\n",
    "\n",
    "    Parameters:\n",
    "    df      : DataFrame\n",
    "    column  : Name of the feature to analyze\n",
    "    target  : Target column (default: 'worldwide_box_office')\n",
    "    \"\"\"\n",
    "\n",
    "    # Skip non-numeric columns to avoid seaborn errors\n",
    "    #if not pd.api.types.is_numeric_dtype(df[column]):\n",
    "     #   print(f\"Skipping '{column}': not numeric\")\n",
    "     #   return\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    plt.suptitle(f\"EDA for: {column}\", fontsize=16)\n",
    "\n",
    "    # 1. Histogram\n",
    "    sns.histplot(df[column], bins=30, ax=axes[0, 0])\n",
    "    axes[0, 0].set_title(\"Histogram\")\n",
    "\n",
    "    # 2. Boxplot\n",
    "    sns.boxplot(x=df[column], ax=axes[0, 1])\n",
    "    axes[0, 1].set_title(\"Boxplot\")\n",
    "\n",
    "    # 3. KDE distribution\n",
    "    sns.kdeplot(df[column], ax=axes[1, 0])\n",
    "    axes[1, 0].set_title(\"KDE Distribution\")\n",
    "\n",
    "    # 4. Relationship with target (scatterplot)\n",
    "    if target in df.columns and pd.api.types.is_numeric_dtype(df[target]):\n",
    "        sns.scatterplot(\n",
    "            x=df[column],\n",
    "            y=df[target],\n",
    "            ax=axes[1, 1],\n",
    "            s=10,\n",
    "            alpha=0.4\n",
    "        )\n",
    "        axes[1, 1].set_title(f\"{column} vs {target}\")\n",
    "    else:\n",
    "        axes[1, 1].axis(\"off\")\n",
    "        axes[1, 1].set_title(\"No numeric target column found\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b5ee3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe with a placeholder for all the created column in the customer_final dataframe. \n",
    "df_EDA_features= [\n",
    "    'metascore',\n",
    "    'userscore',\n",
    "    'production_budget',\n",
    "    'release_month',\n",
    "    'release_year',\n",
    "    'runtime',\n",
    "    'theatre_count',\n",
    "    'worldwide_box_office'       \n",
    "]\n",
    "\n",
    "for col in df_EDA_features:\n",
    "    EDA_plots(movie_data_complete, col, target='worldwide_box_office')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906806f6",
   "metadata": {},
   "source": [
    "### Reflection on the EDA plots per feature\n",
    "\n",
    "- **Metascore**: This feature shows an almost normal distribution, centered around 50-70. There is only one outlier and the scatterplot shows no strong linear relationship between the worldwide box office and metascore. This means that the score of the critics alone is NOT a strong predictor of the box office performance\n",
    "\n",
    "- **Userscore**: This feature shows a right skewed distribution, with most movies receiving an 5 - 8. The boxplot shows some outliers around the 1, these could be people that did not rate the movie honestly. The scatterplot shows that popular movies can have high box office results, but most of them do not. Meaning that userscore alone is also not a strong predictor of box office performance. \n",
    "\n",
    "- **Production_budget**: This features showes a heavily left skewed distribution in which most movies have a production budget of less then 50000000. However, the boxplot showes that there are also movies that have budgets as high as 400,000,000. Interestingly, the scatterplot does show a positive linear relationship. Although not every movie with high production budget succeeds, most of them do! \n",
    "\n",
    "- **Release_month**: The release month is evenly distributed, with some months showing small dips. The scatterplots show some small peaks in the summer and winter. It is not a very strong predictor by itself, but could be useful after one hot encoding in the future.\n",
    "\n",
    "- **Release_year**: The released year show very expected behaviour. More movies have been released in more recent years and the movies tend to earn more in recent years. This is probably due to inflation and general market growth.\n",
    "\n",
    "- **Runtime**: The distribution shows that most movies have a runtime of around 100-150 minutes. There is no clear relationship between runtime and box office performance based on the scatterplot. This is a weak predictor by itself.\n",
    "\n",
    "- **Theatre_count**: This distribution is extremely skewed. Most movies open in very few theatres, but a few movies open in way more theatres. These will probably be blockbuster movies. The scatterplot however shows a very strong positive relationship between the amount of theatres and the worldwide box office. It is also seen that as soon as a movie is showed in more then 4000 theatres, it almost always means more revenue. This feature might need a log transform to handle the skeweness but it will be a very strong predictor.\n",
    "\n",
    "- **Worldwide_box_office**: The target shows a heaviliy left skewed distribution. Meaning that most movies make a few million and some movies make billions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56423f98",
   "metadata": {},
   "source": [
    "### 1.4 Inferential statistics\n",
    "In this chapter, we will move from descriptive statistics to inferential statistics. The numerical features (X1, X2, X3, X6, X7) will be examined with Pearson and Spearmon correlation testing. The categorical features (X4, X5, X8) will be examined with either ANOVA or Kruskal wallis depending on the characterics. \n",
    "\n",
    "### Assummptions reflection: \n",
    "- Pearson correlation assumptions:\n",
    "    1. The relationship between the variables is linear\n",
    "    2. The data is normaly distributed\n",
    "    3. The data is on a continuous scale\n",
    "    4. There should be no extreme outliers \n",
    "\n",
    "- **X1**: Metascore does not violate any assumptions. Although the scatterplot does not show strong linearity, is does show some. \n",
    "- **X2**: Userscore **does** violate assumptions. There are some outliers at the 1, and the distribution is right skewed. Therefore, this is not a good fit for the pearson   correlation test\n",
    "- **X3**: Altough production budget shows linearity, there are extreme outliers and the data is highly left skewed. Therefore it **does** violate assumptions\n",
    "- **X4**: Genre is not a numerical feature, does not fit pearson\n",
    "- **X5**: Release date is not a numerical feature, does not fit pearson \n",
    "- **X6**: Runtime shows a weak linearity and a right skewed distribution and **does** violate the assumptions\n",
    "- **X7**: Theatre count **does** violate the assumption due to the extreme skew.\n",
    "- **X8**: MPAA Rating is not a numerical feature, does not fit pearson\n",
    "- **X9**: Movie Summary is not a numerical feature, does not fit pearson\n",
    "- **X10**: Review Text is not a numerical feature, does not fit pearson\n",
    "\n",
    "Conclusion: the only feature suited for pearson correlation is metascore. However, the target feature violates the assumptions as well. Since pearson correlation is a bivariate test, Pearson is not suited at all. To find correlation but mitigate these violations, Spearman rank correlation is used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dbe0a1",
   "metadata": {},
   "source": [
    "#### 1.4.1: Correlation analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5286aa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Correlation analysis for continuous data\n",
    "# Based on our assumptions reflection, we use Spearman correlation due to:\n",
    "# - Non-normal distributions (skewed data)\n",
    "# - Presence of outliers\n",
    "# - Non-linear relationships\n",
    "\n",
    "# Select all continuous features and the target variable\n",
    "scale_features = [\n",
    "    'metascore',              # X1\n",
    "    'userscore',              # X2\n",
    "    'production_budget',      # X3\n",
    "    'runtime',                # X6\n",
    "    'theatre_count',          # X7\n",
    "    'release_year',           # from X5\n",
    "    'worldwide_box_office'    # Target\n",
    "]\n",
    "\n",
    "# Calculate Spearman correlation (rank-based, robust to outliers and non-linearity)\n",
    "spearman_corr = movie_data_complete[scale_features].corr(method='spearman')\n",
    "\n",
    "print(\"Spearman Correlation Matrix:\")\n",
    "print(\"=\" * 80)\n",
    "print(spearman_corr.round(3))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Focus on correlations with target variable\n",
    "print(\"Spearman Correlations with Worldwide Box Office:\")\n",
    "print(\"=\" * 80)\n",
    "target_corr = spearman_corr['worldwide_box_office'].sort_values(ascending=False)\n",
    "print(target_corr)\n",
    "\n",
    "# Visualize with heatmaps\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Spearman correlation heatmap\n",
    "sns.heatmap(\n",
    "    spearman_corr, \n",
    "    annot=True, \n",
    "    cmap='coolwarm', \n",
    "    fmt=\".2f\", \n",
    "    linewidths=0.5,\n",
    "    center=0,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    ax=axes[0],\n",
    "    cbar_kws={'label': 'Correlation Coefficient'}\n",
    ")\n",
    "axes[0].set_title('Spearman Correlation Matrix\\n(Rank-based, robust to outliers)', fontsize=14, pad=20)\n",
    "\n",
    "# For comparison, also show Pearson (even though assumptions are violated)\n",
    "pearson_corr = movie_data_complete[scale_features].corr(method='pearson')\n",
    "sns.heatmap(\n",
    "    pearson_corr, \n",
    "    annot=True, \n",
    "    cmap='coolwarm', \n",
    "    fmt=\".2f\", \n",
    "    linewidths=0.5,\n",
    "    center=0,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    ax=axes[1],\n",
    "    cbar_kws={'label': 'Correlation Coefficient'}\n",
    ")\n",
    "axes[1].set_title('Pearson Correlation Matrix\\n(For comparison - assumptions violated)', fontsize=14, pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical significance testing for key correlations with target\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Statistical Significance Tests (Spearman) for correlations with target:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "for feature in scale_features:\n",
    "    if feature != 'worldwide_box_office':\n",
    "        # Remove NaN values for the pair\n",
    "        valid_data = movie_data_complete[[feature, 'worldwide_box_office']].dropna()\n",
    "        \n",
    "        if len(valid_data) > 0:\n",
    "            corr, p_value = spearmanr(valid_data[feature], valid_data['worldwide_box_office'])\n",
    "            significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
    "            print(f\"{feature:25s}: ρ = {corr:6.3f}, p = {p_value:.4e} {significance}\")\n",
    "\n",
    "print(\"\\nSignificance levels: *** p<0.001, ** p<0.01, * p<0.05, ns = not significant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5835e9",
   "metadata": {},
   "source": [
    "# Juliusz writes reflection here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c576af5",
   "metadata": {},
   "source": [
    "### 1.5 Handling outliers and missing data\n",
    "\n",
    "In this chapter, we will handle the missing values and outliers before we do any feature engineering. Beacause we have a lot of missing data (see chapter 1.2.1) we need to impute data and remove outliers. Looking at chapter 1.2.1 we see that worldwide_box_office has around 30% missing values and metascore, userscore, production_budget, theathre count, rating and summary have about 60% to as much as 85% missing values. \n",
    "\n",
    "1. First we want to drop the missing rows in release_month and release_year that have since they have a very small amount of missing values.Just for completeness\n",
    "2. Second, we want to impute the data for the features that have some missing values but not that much, like release_date and genre. \n",
    "3. For the features with a lot of missing values, we check to what extend they might have predictive power from the correlation analysis. It could be very interesting to see whether models will have better or worse performance when keeping missing values or dropping them.\n",
    "4. Lasty, we prepare the data for feature engineering and modelling by transforming the skewed data with the log transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ed1c48",
   "metadata": {},
   "source": [
    "### 1.5.1 Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d651a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets take one more look at the missing valiues\n",
    "print(f\"✓ Movie-level dataset: {len(movie_data_complete)} movies with X1-X9 + target\")\n",
    "print(f\"\\n✓ Missing values:\")\n",
    "print(movie_data_complete.isnull().sum())\n",
    "print(f\"\\n✓ Complete cases (no missing values): {movie_data_complete.dropna().shape[0]} movies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930cd243",
   "metadata": {},
   "source": [
    "#### 1.5.1.1 features that have a small amount of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7b3cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step one: drop the missing values from the movie_data_complete file and print the new values as a sanity check\n",
    "movie_data_complete = movie_data_complete.dropna(subset=['release_month','release_year']).copy()\n",
    "print(f\"\\n✓ Missing values:\")\n",
    "print(movie_data_complete[['release_month', 'release_year']].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3a402b",
   "metadata": {},
   "source": [
    "#### 1.5.1.2 features with moderate amount of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b7e072",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we create a new column called primary genre so we can impute te median of the runtime based on their genre\n",
    "#This is basically done because the runtime has more then 2500 missing values and this way we can take the median of the runtime per genre. This will differentiate a little and makes it more precise\n",
    "# 1. gives missing genres a value \"unknown\"\n",
    "movie_data_complete['genre'] = movie_data_complete['genre'].fillna('Unknown')\n",
    "\n",
    "# 2. Create a new column for the primary genre (this is for the imputation of runtime)\n",
    "movie_data_complete['primary_genre'] = movie_data_complete['genre'].str.split(',').str[0].str.strip()\n",
    "\n",
    "# 3. Runtime imputation using primary_genre. We also create a new column with an integer 0 or 1 to tell the model in the future which runtimes are real values and which have been imputed\n",
    "movie_data_complete['runtime_missing'] = movie_data_complete['runtime'].isna().astype(int)\n",
    "\n",
    "movie_data_complete['runtime'] = movie_data_complete.groupby('primary_genre')['runtime'] \\\n",
    "                 .transform(lambda x: x.fillna(x.median())) #fill the empty rows with the median runtime per pprimary genre\n",
    "\n",
    "movie_data_complete.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0c9c14",
   "metadata": {},
   "source": [
    "#### 1.5.1.3 features that have a lot of missing data\n",
    "\n",
    "We want to keep as much information as we can. Also, the features with the most missing data are actually the features that will very likely have the biggest predictive power. This is seen in the correlation analysis. That's why we keep the features and we again create another column with either 0 of 1 for these features telling the model if the data was already there or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2c2563",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_missing_numeric = ['metascore', 'userscore',\n",
    "                        'production_budget', 'theatre_count']\n",
    "\n",
    "#Loop through all the high missing numerical columns to create a new column, and put the median value of that corresponding column in there. Also fopr the Nan's.\n",
    "for col in high_missing_numeric: \n",
    "    # 1. Indicator whether it was missing originally\n",
    "    movie_data_complete[col + '_missing'] = movie_data_complete[col].isna().astype(int)\n",
    "    \n",
    "    # 2. Give the columns the median value\n",
    "    median_val = movie_data_complete[col].median()\n",
    "    \n",
    "    # 3. Fill NaNs with the median\n",
    "    movie_data_complete[col] = movie_data_complete[col].fillna(median_val)\n",
    "\n",
    "#Now give the categorical feature rating an unknown value if it is NaN\n",
    "# Indicator for missing ratings\n",
    "movie_data_complete['rating_missing'] = movie_data_complete['rating'].isna().astype(int)\n",
    "\n",
    "# Impute missing ratings as 'Unknown' and create another column for the indication if it was missing or not\n",
    "movie_data_complete['rating'] = movie_data_complete['rating'].fillna('Unknown')\n",
    "\n",
    "#lastly impute 'Unknown' into the summary column and create a summary_missing column\n",
    "movie_data_complete['summary_missing'] = movie_data_complete['summary'].isna().astype(int)\n",
    "movie_data_complete['summary'] = movie_data_complete['summary'].fillna('Unknown')\n",
    "\n",
    "movie_data_complete.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e318c3",
   "metadata": {},
   "source": [
    "### 1.5.2 Feature transformation to get rid of the CRAZZYYY skew \n",
    "\n",
    "In de EDA plots, it is clearly seen that worldwide box office (the target), product budget and runtime have very skewed distributions. These get a log transformation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f2078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log transformations on all the skewed features\n",
    "movie_data_complete['worldwide_box_office_log'] = np.log1p(movie_data_complete['worldwide_box_office'])\n",
    "movie_data_complete['production_budget_log']    = np.log1p(movie_data_complete['production_budget'])\n",
    "movie_data_complete['theatre_count_log']        = np.log1p(movie_data_complete['theatre_count'])\n",
    "movie_data_complete['runtime_log']        = np.log1p(movie_data_complete['runtime'])\n",
    "\n",
    "movie_data_complete.head()\n",
    "\n",
    "# Now lets reuse that STUNNING EDA plot function to see if it worked\n",
    "skewed_features = [\n",
    "    'worldwide_box_office_log',\n",
    "    'production_budget_log',\n",
    "    'theatre_count_log',\n",
    "    'runtime_log'       \n",
    "]\n",
    "\n",
    "for col in skewed_features:\n",
    "    EDA_plots(movie_data_complete, col, target='worldwide_box_office')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba0646b",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering\n",
    "\n",
    "This section transforms raw variables into model-ready features. Feature engineering is critical because:\n",
    "- **Tree-based models** (Random Forest, XGBoost) work best with properly encoded categorical variables\n",
    "- **Multi-label features** (genres) require special handling to preserve all information\n",
    "- **Temporal patterns** in release dates can capture seasonal box office trends\n",
    "\n",
    "**Features to Engineer:**\n",
    "| Variable | Type | Transformation |\n",
    "|----------|------|----------------|\n",
    "| Genre (X4) | Multi-label categorical | MultiLabelBinarizer → binary columns |\n",
    "| MPAA Rating (X8) | Categorical | Clean + One-hot encode |\n",
    "| Release Date (X5) | Date | Extract month, year, season indicators |\n",
    "| Production Budget (X3) | Numeric (skewed) | Log transform |\n",
    "| Box Office (Target) | Numeric (skewed) | Log transform → Tertile classification |\n",
    "\n",
    "**Data Leakage Considerations:**\n",
    "We exclude the following features from modeling as they contain information only available *after* a movie's release:\n",
    "- `opening_weekend`: Directly correlated with final box office\n",
    "- `theatre_count`: Determined by distributor based on expected performance\n",
    "- `avg_run_per_theatre`: Post-release metric\n",
    "\n",
    "These would \"leak\" the target variable into features, making the model unrealistically accurate but useless for pre-release predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cab646",
   "metadata": {},
   "source": [
    "# categoricals vs target (genre / rating / release year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d7cad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    movie_data_complete\n",
    "    .groupby(\"genre\", observed=False)[\"worldwide_box_office\"]\n",
    "    .agg([\"count\", \"mean\", \"median\"])\n",
    "    .sort_values(\"mean\", ascending=False)\n",
    "    .head(10)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71bf86e",
   "metadata": {},
   "source": [
    "### Categorical variables and sales (genres)\n",
    "\n",
    "We grouped the data by the full `genre` string and computed the **count**, **mean** and **median** of `worldwide_box_office` for each group.\n",
    "\n",
    "What we see in the top rows:\n",
    "\n",
    "- The highest average box office comes from **long genre combinations** such as  \n",
    "  `Adventure,Fantasy,Comedy,Romance,Family,Musical` or  \n",
    "  `Action,Adventure,Comedy,Crime,Animation,Family`.  \n",
    "  These are all big, broad, family-oriented combinations (adventure, fantasy, comedy, family, animation).\n",
    "\n",
    "- For most of these top genre combinations the **count is only 1**.  \n",
    "  This means the high mean is driven by a **single blockbuster** in that category, so we cannot generalise too much from these rows.\n",
    "\n",
    "- The only combination in the top 10 with more observations is  \n",
    "  `Adventure,Mystery,Fantasy,Family` (5 movies) with an average box office of roughly **8.9**.  \n",
    "  This confirms that large adventure/fantasy family movies are among the strongest commercial performers.\n",
    "\n",
    "Implications for modelling:\n",
    "\n",
    "- Genre clearly matters, but using the **full genre string** leads to many very rare combinations (mostly count = 1).  \n",
    "- Before using genre in our models, it will be better to **simplify it**, for example by extracting a `main_genre` or grouping genres into a smaller number of high-level categories. This reduces sparsity and makes the model easier to interpret.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e93a742",
   "metadata": {},
   "source": [
    "# outliers and log-transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea22165",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "sns.histplot(movie_data_complete[\"worldwide_box_office\"].dropna(), ax=axes[0], bins=50)\n",
    "axes[0].set_title(\"Worldwide box office (raw)\")\n",
    "\n",
    "sns.histplot(\n",
    "    np.log1p(movie_data_complete[\"worldwide_box_office\"].dropna()),\n",
    "    ax=axes[1],\n",
    "    bins=50\n",
    ")\n",
    "axes[1].set_title(\"Worldwide box office (log-transformed)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929cb294",
   "metadata": {},
   "source": [
    "### Outliers and transformations\n",
    "\n",
    "The two histograms compare the distribution of `worldwide_box_office` **before** and **after** a log transformation.\n",
    "\n",
    "**Raw scale (left plot)**  \n",
    "- The distribution is extremely right-skewed.  \n",
    "- Most movies earn relatively modest amounts, and a small number of blockbusters reach very high box office values.  \n",
    "- The long tail on the right makes it hard for a model to treat “normal” movies and extreme hits in a balanced way, because the loss will be dominated by a few very large values.\n",
    "\n",
    "**Log-transformed scale (right plot)**  \n",
    "- After applying `log(1 + worldwide_box_office)`, the histogram becomes much more spread out and closer to a bell-shaped distribution.  \n",
    "- The blockbusters are still at the high end, but they are no longer so extreme compared to the bulk of the data.  \n",
    "- This makes patterns easier to see and typically leads to a more stable regression model.\n",
    "\n",
    "**Implication for modelling**  \n",
    "For these reasons, it is reasonable to train our models on  \n",
    "`log(1 + worldwide_box_office)` instead of the raw box office.  \n",
    "We can always transform predictions back to the original scale by applying the inverse transformation `exp(pred) - 1`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c22a4e1",
   "metadata": {},
   "source": [
    "# Missing Values\n",
    "\n",
    "### Missing values: final decisions\n",
    "\n",
    "Based on the missing value inspection:\n",
    "\n",
    "- We drop completely empty or irrelevant columns (e.g. `Unnamed: 8`).\n",
    "- **For our modelling dataset we keep only movies with a non-missing `worldwide_box_office` and `production_budget`, because both are essential for our research question.**\n",
    "- For other features with moderate missingness (e.g. `runtime`), we either:\n",
    "  - drop the rows (for smaller amounts of missingness), or\n",
    "  - apply simple imputation (e.g. median or most frequent value) in a later preprocessing step.\n",
    "\n",
    "These choices are based on the trade-off between data quality and keeping enough observations for training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ba1a98",
   "metadata": {},
   "source": [
    "# 2.1 Genre Multi-Label Encoding (X4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309ff2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Step 1: Split genres into lists\n",
    "movie_data_complete['genre_list'] = movie_data_complete['genre'].apply(\n",
    "    lambda x: [g.strip() for g in str(x).split(',')]\n",
    ")\n",
    "\n",
    "print(f\"\\n1. Found {movie_data_complete['genre'].nunique()} unique genre combinations\")\n",
    "\n",
    "# Step 2: Apply MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "genre_encoded = mlb.fit_transform(movie_data_complete['genre_list'])\n",
    "\n",
    "genre_df = pd.DataFrame(\n",
    "    genre_encoded,\n",
    "    columns=[f'genre_{genre}' for genre in mlb.classes_],\n",
    "    index=movie_data_complete.index\n",
    ")\n",
    "\n",
    "print(f\"\\n2. Created {len(mlb.classes_)} binary genre columns\")\n",
    "print(f\"\\nTop 5 genres:\")\n",
    "print(genre_df.sum().sort_values(ascending=False).head(5))\n",
    "\n",
    "# Step 3: Merge back\n",
    "movie_data_complete = pd.concat([movie_data_complete, genre_df], axis=1)\n",
    "\n",
    "print(f\"\\n✓ Genre encoding complete! Shape: {movie_data_complete.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e85d31f",
   "metadata": {},
   "source": [
    "# 2.2 MPAA Rating Processing (X8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99208cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Check current state\n",
    "print(f\"\\n1. Before cleaning:\")\n",
    "print(movie_data_complete['rating'].value_counts().head(10))\n",
    "\n",
    "# Step 2: Clean and standardize\n",
    "movie_data_complete['rating_clean'] = movie_data_complete['rating'].str.replace('| ', '', regex=False)\n",
    "\n",
    "# Step 3: Map to standard MPAA (including TV ratings and typos)\n",
    "rating_mapping = {\n",
    "    'G': 'G', 'PG': 'PG', 'PG-13': 'PG-13', 'PG--13': 'PG-13',\n",
    "    'R': 'R', 'NC-17': 'NC-17',\n",
    "    'Not Rated': 'Not Rated', 'Unrated': 'Not Rated', 'NR': 'Not Rated', 'Unknown': 'Not Rated',\n",
    "    # Map TV/other ratings to Not Rated\n",
    "    'TV-MA': 'Not Rated', 'TV-14': 'Not Rated', 'TV-PG': 'Not Rated', 'TV-G': 'Not Rated',\n",
    "    'Approved': 'Not Rated', 'Open': 'Not Rated', 'MA-17': 'Not Rated'\n",
    "}\n",
    "\n",
    "movie_data_complete['rating_clean'] = movie_data_complete['rating_clean'].map(\n",
    "    lambda x: rating_mapping.get(x, 'Not Rated')\n",
    ")\n",
    "\n",
    "print(f\"\\n2. After standardization (MPAA only):\")\n",
    "print(movie_data_complete['rating_clean'].value_counts())\n",
    "\n",
    "movie_data_complete = movie_data_complete.loc[:, ~movie_data_complete.columns.duplicated()]\n",
    "print(f\"After deduplication: {movie_data_complete.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7973180",
   "metadata": {},
   "source": [
    "# One-Hot vs Ordinal Encoding\n",
    "\n",
    "For tree-based models (Random Forest, XGBoost, LightGBM), we recommend ONE-HOT ENCODING.\n",
    "\n",
    "Reasoning:\n",
    "1. **Tree-based models handle categorical features well**: They can split on any value,\n",
    "   so ordinal encoding creates an artificial order (G < PG < PG-13 < R < NC-17).\n",
    "\n",
    "2. **MPAA ratings are NOT strictly ordinal for box office prediction**:\n",
    "   - While ratings indicate restrictiveness (age restrictions), this doesn't translate \n",
    "     to a linear relationship with box office revenue.\n",
    "   - R-rated movies can outperform PG-13 movies (e.g., R-rated blockbusters like \n",
    "     Deadpool, Joker), so the relationship is not monotonic.\n",
    "   - G and PG ratings might perform differently despite being adjacent in restrictiveness.\n",
    "\n",
    "3. **One-hot encoding preserves flexibility**:\n",
    "   - Allows the model to learn independent relationships for each rating category.\n",
    "   - Tree models can easily combine categories if needed (e.g., \"G or PG\" splits).\n",
    "   - No risk of imposing incorrect ordinal assumptions.\n",
    "\n",
    "4. **Dimensionality is not a concern**:\n",
    "   - Only 6 categories (G, PG, PG-13, R, NC-17, Not Rated) → only 5 dummy columns.\n",
    "   - Tree-based models are not affected by multicollinearity like linear models.\n",
    "\n",
    "CONCLUSION: We'll use ONE-HOT ENCODING (pd.get_dummies) for MPAA ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948a8435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Apply one-hot encoding\n",
    "rating_encoded = pd.get_dummies(\n",
    "    movie_data_complete['rating_clean'],\n",
    "    prefix='rating',\n",
    "    drop_first=False  # Keep all categories for interpretability\n",
    ")\n",
    "\n",
    "print(f\"\\n3. One-hot encoding applied:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Created {rating_encoded.shape[1]} binary rating columns:\")\n",
    "print(rating_encoded.columns.tolist())\n",
    "print(f\"\\nRating distribution:\")\n",
    "print(rating_encoded.sum().sort_values(ascending=False))\n",
    "\n",
    "# Step 4: Merge back to main dataframe\n",
    "movie_data_complete = pd.concat([movie_data_complete, rating_encoded], axis=1)\n",
    "\n",
    "print(f\"\\n✓ Rating encoding complete! Shape: {movie_data_complete.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48653a77",
   "metadata": {},
   "source": [
    "# 2.3 Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d21cd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_cols = [col for col in movie_data_complete.columns if col.startswith('genre_')]\n",
    "rating_cols = [col for col in movie_data_complete.columns if col.startswith('rating_')]\n",
    "\n",
    "print(f\"\\n✓ Genre features: {len(genre_cols)} binary columns\")\n",
    "print(f\"✓ Rating features: {len(rating_cols)} binary columns\")\n",
    "print(f\"✓ Total columns: {movie_data_complete.shape[1]}\")\n",
    "print(f\"\\n✓ Both features are encoded and ready for modeling!\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"SAMPLE: Original vs Encoded Features\")\n",
    "print(\"-\" * 80)\n",
    "sample_cols = ['genre', 'rating_clean'] + genre_cols[:3] + rating_cols[:3]\n",
    "print(movie_data_complete[sample_cols].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833b191b",
   "metadata": {},
   "source": [
    "### Reflection: Genre and Rating Feature Engineering\n",
    "\n",
    "**Genre Multi-Label Encoding (X4):**  \n",
    "We split the comma-separated `genre` column into lists and applied `MultiLabelBinarizer`, creating 32 binary genre columns. This treats each genre independently, allowing movies to have multiple genres simultaneously (e.g., \"Action,Adventure,Sci-Fi\"). For tree-based models, this approach enables learning both individual genre effects and genre combination patterns that drive box office performance.\n",
    "\n",
    "**MPAA Rating One-Hot Encoding (X8):**  \n",
    "We standardized the `rating` column to official MPAA categories (G, PG, PG-13, R, NC-17, Not Rated) by:\n",
    "- Removing the \"| \" prefix\n",
    "- Mapping TV ratings (TV-MA, TV-14, etc.) to \"Not Rated\"\n",
    "- Fixing typos (PG--13 → PG-13)\n",
    "- Consolidating all non-standard values to \"Not Rated\"\n",
    "\n",
    "Then applied one-hot encoding to create 6 binary rating columns.\n",
    "\n",
    "**Why One-Hot Encoding for Rating:**  \n",
    "We used one-hot encoding rather than ordinal because:\n",
    "- MPAA restrictiveness doesn't correlate linearly with box office (R-rated *Deadpool* earned $783M, outperforming many PG-13 films)\n",
    "- Tree-based models learn non-monotonic relationships better with categorical encoding\n",
    "- Only 6 rating categories means minimal dimensionality impact\n",
    "\n",
    "**Limitation:** 83% of ratings are \"Not Rated\" due to missing values, which reduces this feature's predictive power. The `rating_missing` indicator flag helps the model distinguish truly missing ratings from known values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonality_header",
   "metadata": {},
   "source": [
    "## 2.4 Seasonality Features (X5 - Release Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonality_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# release_month and release_year already extracted in section 1.2.3\n",
    "print(f\"\\n1. Release date features already extracted:\")\n",
    "print(f\"   - release_month: {movie_data_complete['release_month'].nunique()} unique months\")\n",
    "print(f\"   - release_year: {movie_data_complete['release_year'].nunique()} unique years\")\n",
    "\n",
    "# Step 2: Create seasonal categories\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:  # 9, 10, 11\n",
    "        return 'Fall'\n",
    "\n",
    "movie_data_complete['season'] = movie_data_complete['release_month'].apply(get_season)\n",
    "\n",
    "print(f\"\\n2. Created season categories:\")\n",
    "print(movie_data_complete['season'].value_counts())\n",
    "\n",
    "# Step 3: Create holiday/peak season indicators\n",
    "movie_data_complete['summer_release'] = (movie_data_complete['release_month'].isin([5, 6, 7, 8])).astype(int)\n",
    "movie_data_complete['holiday_release'] = (movie_data_complete['release_month'].isin([11, 12])).astype(int)\n",
    "\n",
    "print(f\"\\n3. Created peak season indicators:\")\n",
    "print(f\"   - Summer releases (May-Aug): {movie_data_complete['summer_release'].sum()}\")\n",
    "print(f\"   - Holiday releases (Nov-Dec): {movie_data_complete['holiday_release'].sum()}\")\n",
    "\n",
    "# Step 4: One-hot encode season (alternative to month)\n",
    "season_encoded = pd.get_dummies(movie_data_complete['season'], prefix='season')\n",
    "movie_data_complete = pd.concat([movie_data_complete, season_encoded], axis=1)\n",
    "\n",
    "print(f\"\\n4. One-hot encoded seasons:\")\n",
    "print(f\"   Created {season_encoded.shape[1]} season columns: {season_encoded.columns.tolist()}\")\n",
    "\n",
    "print(f\"\\n✓ Seasonality features complete! Shape: {movie_data_complete.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7821ef",
   "metadata": {},
   "source": [
    "## 2.5 Production Budget Transformation (X3)\n",
    "\n",
    "Like box office revenue, production budgets are heavily right-skewed (most films are low-budget; a few blockbusters cost $200M+). We apply the same log transformation for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonality_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze seasonality impact on box office\n",
    "\n",
    "# Group by season\n",
    "season_stats = movie_data_complete.groupby('season')['worldwide_box_office'].agg([\n",
    "    'count', 'mean', 'median'\n",
    "]).sort_values('mean', ascending=False)\n",
    "\n",
    "print(f\"\\nBox office by season:\")\n",
    "print(season_stats)\n",
    "\n",
    "# Group by month\n",
    "month_stats = movie_data_complete.groupby('release_month')['worldwide_box_office'].agg([\n",
    "    'count', 'mean', 'median'\n",
    "]).sort_values('mean', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 5 months by average box office:\")\n",
    "print(month_stats.head(5))\n",
    "\n",
    "# Compare peak vs non-peak\n",
    "print(f\"\\nPeak season comparison:\")\n",
    "print(f\"Summer releases - Mean: ${movie_data_complete[movie_data_complete['summer_release']==1]['worldwide_box_office'].mean():,.0f}\")\n",
    "print(f\"Non-summer releases - Mean: ${movie_data_complete[movie_data_complete['summer_release']==0]['worldwide_box_office'].mean():,.0f}\")\n",
    "print(f\"\\nHoliday releases - Mean: ${movie_data_complete[movie_data_complete['holiday_release']==1]['worldwide_box_office'].mean():,.0f}\")\n",
    "print(f\"Non-holiday releases - Mean: ${movie_data_complete[movie_data_complete['holiday_release']==0]['worldwide_box_office'].mean():,.0f}\")\n",
    "\n",
    "# Log-transform production budget\n",
    "movie_data_complete['production_budget_log'] = np.log1p(movie_data_complete['production_budget'])\n",
    "\n",
    "print(f\"Production budget - Original range: ${movie_data_complete['production_budget'].min():,.0f} to ${movie_data_complete['production_budget'].max():,.0f}\")\n",
    "print(f\"Production budget - Log range: {movie_data_complete['production_budget_log'].min():.2f} to {movie_data_complete['production_budget_log'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonality_reflection",
   "metadata": {},
   "source": [
    "### Reflection: Seasonality Feature Engineering\n",
    "\n",
    "**Features Created from Release Date (X5):**  \n",
    "We extracted temporal features from the `releasedate` column to capture seasonal patterns in box office performance:\n",
    "\n",
    "1. **Basic temporal features** (already created in section 1.2.3):\n",
    "   - `release_month`: Numeric month (1-12)\n",
    "   - `release_year`: Year of release (for inflation/market growth trends)\n",
    "\n",
    "2. **Season categories**:\n",
    "   - Created `season` column: Winter, Spring, Summer, Fall\n",
    "   - One-hot encoded to 4 binary columns: `season_Winter`, `season_Spring`, `season_Summer`, `season_Fall`\n",
    "\n",
    "3. **Peak season indicators**:\n",
    "   - `summer_release`: Binary flag for May-August releases (blockbuster season)\n",
    "   - `holiday_release`: Binary flag for November-December releases (holiday season)\n",
    "\n",
    "**Why These Features Matter:**  \n",
    "The movie industry shows strong seasonality:\n",
    "- **Summer (May-Aug)**: Studios release big-budget blockbusters targeting families and teens on vacation\n",
    "- **Holiday (Nov-Dec)**: Award contenders and family films capitalize on Thanksgiving/Christmas breaks\n",
    "- **Dump months (Jan-Feb, Aug-Sep)**: Lower-budget films with weaker box office potential\n",
    "\n",
    "From the EDA in section 1.3, we observed peaks in summer and winter months, confirming this pattern.\n",
    "\n",
    "**Encoding Strategy:**  \n",
    "We provide multiple representations for flexibility:\n",
    "- `release_month` (numeric): Captures month-specific trends\n",
    "- `season_*` (one-hot): Groups months into broader seasonal patterns\n",
    "- `summer_release`, `holiday_release` (binary): Direct indicators for peak release windows\n",
    "\n",
    "Tree-based models can choose which representation works best for splits. Keeping `release_year` as numeric (not one-hot) preserves the temporal trend (inflation, market growth over time)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "target_header",
   "metadata": {},
   "source": [
    "## 2.5 Target Variable: Sales Tiers (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "target_creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "\n",
    "# Step 1: Examine box office distribution and skewness\n",
    "print(f\"\\n1. Worldwide box office distribution:\")\n",
    "print(f\"   Count: {movie_data_complete['worldwide_box_office'].count()}\")\n",
    "print(f\"   Mean: ${movie_data_complete['worldwide_box_office'].mean():,.0f}\")\n",
    "print(f\"   Median: ${movie_data_complete['worldwide_box_office'].median():,.0f}\")\n",
    "print(f\"   Min: ${movie_data_complete['worldwide_box_office'].min():,.0f}\")\n",
    "print(f\"   Max: ${movie_data_complete['worldwide_box_office'].max():,.0f}\")\n",
    "print(f\"   Std: ${movie_data_complete['worldwide_box_office'].std():,.0f}\")\n",
    "\n",
    "# Check skewness\n",
    "skewness = skew(movie_data_complete['worldwide_box_office'].dropna())\n",
    "print(f\"   Skewness: {skewness:.2f} (heavily right-skewed if > 1)\")\n",
    "\n",
    "# Step 2: Use LOG-TRANSFORMED data for tier boundaries\n",
    "# This handles outliers better and creates more meaningful business segments\n",
    "print(f\"\\n2. Creating tiers using LOG-TRANSFORMED box office:\")\n",
    "print(f\"   Rationale: Log transformation reduces impact of extreme outliers\")\n",
    "print(f\"              and creates more balanced tier separations\")\n",
    "\n",
    "# Calculate tertiles on LOG scale\n",
    "log_q33 = movie_data_complete['worldwide_box_office_log'].quantile(0.33)\n",
    "log_q67 = movie_data_complete['worldwide_box_office_log'].quantile(0.67)\n",
    "\n",
    "# Convert back to original scale for interpretability\n",
    "q33_actual = np.expm1(log_q33)  # inverse of log1p\n",
    "q67_actual = np.expm1(log_q67)\n",
    "\n",
    "print(f\"\\n3. Sales tier boundaries (based on log tertiles):\")\n",
    "print(f\"   Low tier:    < ${q33_actual:,.0f} (bottom 33%)\")\n",
    "print(f\"   Medium tier: ${q33_actual:,.0f} - ${q67_actual:,.0f} (middle 34%)\")\n",
    "print(f\"   High tier:   > ${q67_actual:,.0f} (top 33%)\")\n",
    "\n",
    "# Step 3: Create sales tier using log-based boundaries\n",
    "def classify_sales_log(box_office_log):\n",
    "    if box_office_log < log_q33:\n",
    "        return 'Low'\n",
    "    elif box_office_log < log_q67:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "movie_data_complete['sales_tier'] = movie_data_complete['worldwide_box_office_log'].apply(classify_sales_log)\n",
    "\n",
    "print(f\"\\n4. Sales tier distribution:\")\n",
    "tier_counts = movie_data_complete['sales_tier'].value_counts().sort_index()\n",
    "print(tier_counts)\n",
    "print(f\"\\n   Balance check: {tier_counts.min()}/{tier_counts.max()} = {tier_counts.min()/tier_counts.max():.2%} (should be ~100%)\")\n",
    "\n",
    "# Step 5: Show actual box office ranges per tier\n",
    "print(f\"\\n5. Actual box office ranges per tier (original scale):\")\n",
    "for tier in ['Low', 'Medium', 'High']:\n",
    "    tier_data = movie_data_complete[movie_data_complete['sales_tier'] == tier]['worldwide_box_office']\n",
    "    print(f\"   {tier:7s}: ${tier_data.min():>12,.0f} - ${tier_data.max():>12,.0f} (median: ${tier_data.median():>12,.0f})\")\n",
    "\n",
    "# Step 6: Create numeric encoding for target (for some models)\n",
    "tier_mapping = {'Low': 0, 'Medium': 1, 'High': 2}\n",
    "movie_data_complete['sales_tier_encoded'] = movie_data_complete['sales_tier'].map(tier_mapping)\n",
    "\n",
    "print(f\"\\n6. Numeric encoding created: Low=0, Medium=1, High=2\")\n",
    "print(f\"\\n✓ Target variable created! Shape: {movie_data_complete.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "target_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics by tier\n",
    "tier_stats = movie_data_complete.groupby('sales_tier')['worldwide_box_office'].agg([\n",
    "    'count', 'mean', 'median', 'min', 'max'\n",
    "])\n",
    "print(f\"\\nBox office statistics by tier:\")\n",
    "print(tier_stats)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distribution of sales tiers\n",
    "tier_counts = movie_data_complete['sales_tier'].value_counts().sort_index()\n",
    "axes[0].bar(tier_counts.index, tier_counts.values, color=['#d62728', '#ff7f0e', '#2ca02c'])\n",
    "axes[0].set_title('Sales Tier Distribution', fontsize=14)\n",
    "axes[0].set_xlabel('Sales Tier')\n",
    "axes[0].set_ylabel('Number of Movies')\n",
    "for i, v in enumerate(tier_counts.values):\n",
    "    axes[0].text(i, v + 100, str(v), ha='center', va='bottom')\n",
    "\n",
    "# Box office by tier (log scale for visibility)\n",
    "movie_data_complete.boxplot(column='worldwide_box_office_log', by='sales_tier', \n",
    "                            ax=axes[1], patch_artist=True)\n",
    "axes[1].set_title('Box Office Distribution by Tier (log scale)', fontsize=14)\n",
    "axes[1].set_xlabel('Sales Tier')\n",
    "axes[1].set_ylabel('Log(Worldwide Box Office)')\n",
    "plt.suptitle('')  # Remove auto-title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Sales tier analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "target_reflection",
   "metadata": {},
   "source": [
    "### Reflection: Target Variable Creation\n",
    "\n",
    "**Sales Tier Classification:**  \n",
    "Based on the research questions (sections 0 and 0.1), this project aims to predict whether a movie will achieve **low, medium, or high sales**. We created the classification target variable `sales_tier` by splitting `worldwide_box_office` into tertiles **using log-transformed values**.\n",
    "\n",
    "**Why Log-Based Tertiles:**  \n",
    "The EDA (section 1.3) revealed that `worldwide_box_office` is **heavily right-skewed** (most movies earn modest amounts, few earn billions). Standard tertiles on raw values would:\n",
    "- Place the boundary between Low/Medium at a very low value (e.g., $2-3M)\n",
    "- Create a \"High\" tier dominated by extreme outliers (blockbusters earning >$500M)\n",
    "- Result in poor separation between tiers for meaningful business decisions\n",
    "\n",
    "Using **log-transformed tertiles** addresses this:\n",
    "1. **Reduces outlier impact**: Log transformation compresses the scale, preventing blockbusters from distorting boundaries\n",
    "2. **Creates meaningful segments**: Boundaries reflect proportional differences in performance, not absolute dollar gaps\n",
    "3. **Balanced classes**: Ensures ~33% of movies in each tier\n",
    "4. **Aligns with business reality**: Movie industry thinks in terms of magnitude (e.g., \"10x return\"), not absolute differences\n",
    "\n",
    "**Tier Definitions (Log-Based):**\n",
    "- **Low tier** (0): Bottom 33% on log scale - Movies underperforming relative to the distribution\n",
    "- **Medium tier** (1): Middle 34% on log scale - Moderately successful movies\n",
    "- **High tier** (2): Top 33% on log scale - Strong commercial performers and blockbusters\n",
    "\n",
    "\n",
    "**Statistical Validity:**\n",
    "- **Skewness handled**: Log transformation normalizes distribution before splitting\n",
    "- **Class balance**: Tertiles ensure balanced classes for unbiased model training\n",
    "- **Interpretability**: Converting boundaries back to original scale shows actual dollar ranges\n",
    "- **Robustness**: Log-based approach is standard for skewed financial data (box office, revenue, sales)\n",
    "\n",
    "**Target Variables Created:**\n",
    "1. `sales_tier`: Categorical (Low/Medium/High) - primary target for classification\n",
    "2. `sales_tier_encoded`: Numeric (0/1/2) - for models requiring numeric targets\n",
    "3. `worldwide_box_office`: Continuous - kept for reference\n",
    "4. `worldwide_box_office_log`: Log-transformed - used for tier creation\n",
    "\n",
    "**Research Alignment:**  \n",
    "This approach directly supports:\n",
    "- **Business question**: Predict performance tiers to allocate marketing budget efficiently\n",
    "- **Subquestion 1**: Analyze feature relationships with sales tiers\n",
    "- **Subquestion 2**: Train ML models to predict tier membership\n",
    "- **Subquestion 4**: SHAP analysis of high vs low sales drivers\n",
    "- **Subquestion 5**: Identify market segments (clusters) associated with each tier\n",
    "\n",
    "**Class Balance Verification:**  \n",
    "The log-based tertile approach ensures approximately equal class sizes (~33% each), which is crucial for:\n",
    "- Avoiding model bias toward majority class\n",
    "- Accurate evaluation metrics (precision, recall, F1-score)\n",
    "- Fair SHAP value comparisons across tiers\n",
    "- Meaningful feature importance interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d45f150",
   "metadata": {},
   "source": [
    "## 2.7 Feature Engineering Summary\n",
    "\n",
    "### Final Feature Set for Modeling\n",
    "\n",
    "| Category | Features | Count | Notes |\n",
    "|----------|----------|-------|-------|\n",
    "| **Target** | `sales_tier_encoded` | 1 | 0=Low, 1=Medium, 2=High |\n",
    "| **Numeric** | `metascore`, `userscore`, `runtime`, `production_budget_log`, `release_year` | 5 | Continuous features |\n",
    "| **Genre** | `genre_*` | 32 | Binary (multi-label encoded) |\n",
    "| **Rating** | `rating_*` | 6 | Binary (one-hot encoded) |\n",
    "| **Seasonality** | `season_*`, `summer_release`, `holiday_release` | 6 | Binary indicators |\n",
    "\n",
    "**Total features for modeling:** ~49 (excluding intermediate/raw columns)\n",
    "\n",
    "### Features Excluded from Modeling\n",
    "- `worldwide_box_office`, `worldwide_box_office_log`: Target variable (used to create tiers)\n",
    "- `opening_weekend`, `theatre_count`, `avg_run_per_theatre`: Data leakage risk\n",
    "- `genre`, `rating`, `rating_clean`, `season`: Raw categorical columns (encoded versions used instead)\n",
    "- `title`, `url`, `movie_id`: Identifiers, not predictive\n",
    "\n",
    "### Next Steps\n",
    "1. **Train/Validation/Test Split** - Stratified by `sales_tier`\n",
    "2. **Feature Scaling** - StandardScaler for numeric features (fit on train only)\n",
    "3. **Handle remaining missing values** - Imputation strategy for `runtime`, `userscore`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
